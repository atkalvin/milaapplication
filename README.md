# Application  
This repository is for my application at Mila / U de M.

# 1. Introduction

This project implements a simple yet complete recommendation system using the **MovieLens 100k dataset**.  
It was built as a code sample to demonstrate essential machine learning skills relevant to applied research and engineering roles, including:

- **Data loading and preprocessing**  
- **Baseline recommendation models (User–User KNN)**  
- **Matrix factorization using SVD**  
- **Evaluation using Precision@K**  
- **Modular, clean project structure**

The objective is not to achieve state-of-the-art accuracy, but to show a clear, reproducible pipeline and an analysis of how classical recommender models behave on sparse rating datasets.

# 2. Dataset

This project uses the **MovieLens 100k** dataset, a widely used benchmark for evaluating collaborative filtering models.

## 2.1 Description
- **100,000 ratings**
- **943 users**
- **1,682 movies**
- Ratings range from 1 to 5.
- The dataset is **highly sparse**, with most users having rated only a small fraction of all movies.

## 2.2 Why MovieLens?
MovieLens is a standard dataset in the recommendation systems literature because:

- It is small enough to run experiments quickly.
- It reflects real sparsity patterns found in user-item interactions.
- It provides a realistic but manageable challenge for baseline and matrix factorization models.
- It allows direct comparison between different recommendation approaches.

## 2.3 Preprocessing
The dataset is loaded and pivoted into a **user–item matrix** using `data_loader.py`.  
Missing ratings are filled with zeros for SVD training.  

For evaluation, only **positive ratings** (typically ≥ 4) are considered as "relevant items", following common practice in implicit-feedback scenarios.

# 3. Models

This project implements two classic families of collaborative filtering models:

---

## 3.1 Baseline: User–User KNN

The baseline model computes similarity between users based on their rating vectors.  
Recommendations are generated by aggregating ratings from the most similar users.

### Strengths
- Very easy to implement and interpret  
- Provides a meaningful baseline for comparison  

### Limitations
- Performs poorly on sparse datasets like MovieLens 100k  
- Users often have little overlap in their rated movies  
- Sensitive to noise and missing data  
- Does not capture latent patterns  

This model is included to establish a simple reference point before moving to a more powerful approach.

---

## 3.2 Matrix Factorization (SVD)

The second model uses **Singular Value Decomposition (SVD)** to factorize the user–item matrix into lower-dimensional latent factors:

- Each user is represented by a latent vector  
- Each movie is represented by a latent vector  
- Predicted ratings are obtained through vector similarity

This approach captures latent structure (e.g., genre preferences, style affinity) that KNN cannot detect.

### Strengths
- More scalable and expressive than KNN  
- Captures hidden patterns in user behavior  
- Standard baseline in many recommender system pipelines  

### Limitations
- Still impacted by sparsity  
- No regularization in this simple implementation  
- Performance limited by classical SVD (not SVD++, ALS, etc.)

The goal is not to reproduce industrial-level models, but to demonstrate a clean implementation and comparison of two well-known algorithms.

# 4. Evaluation

The models are evaluated using the **leave-one-out** strategy commonly used for recommendation systems:

1. For each user, one positively-rated item (rating ≥ 4) is temporarily removed.  
2. The model must recommend items the user has not seen.  
3. If the hidden item appears in the Top-K recommendations, it is counted as a hit.

This leads to the **Precision@K** metric:

\[
\text{Precision@K} = \frac{\text{# relevant items in top-K}}{K}
\]

---

## Why is Precision@K low on MovieLens?

MovieLens 100k is **highly sparse**:

- 943 users  
- 1,682 movies  
- Most users rate only a small number of movies  

Finding the **exact** hidden item among thousands of possibilities is extremely difficult, especially with simple classical models such as KNN or basic SVD.

This is expected behavior and consistent with results reported in the literature for non-regularized baselines.

The goal is not to maximize this metric, but to analyze and compare the relative behavior of the models.

# 6. Results Summary

The table below summarizes the performance of both models using Precision@K under the leave-one-out setup.

*(Values are illustrative and depend on sampling size; exact numbers appear in the notebook.)*

| Model              | Precision@5 | Precision@20 | Notes |
|--------------------|-------------|--------------|-------|
| User–User KNN      | ~0.000      | ~0.001       | Very weak baseline on sparse data |
| SVD (20 factors)   | ~0.000–0.005 | ~0.005–0.02 | Captures latent structure; still limited by matrix sparsity |

Although the absolute Precision@K remains low, the SVD model consistently performs better than the KNN baseline.
This confirms the importance of latent factor models for recommendation tasks.